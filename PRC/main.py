import argparse
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import os
import logging
import time
from datetime import datetime, timedelta
import hmac
import base64
import hashlib
import urllib.parse
import requests

from playwright.sync_api import sync_playwright, BrowserContext, Page
from . import id_map
from . import ragflow_api

try:
    # ä½œä¸ºåŒ…è¿è¡Œï¼špython -m auto_downloader.PRC.main
    from .utils import ensure_dir, sanitize_name, file_exists_nonempty, load_json, save_json, backoff_delays, sleep
except Exception:
    # ç›´æ¥è„šæœ¬è¿è¡Œï¼špython auto_downloader/PRC/main.py
    import pathlib as _pathlib
    import sys as _sys
    _sys.path.append(str(_pathlib.Path(__file__).resolve().parent))
    from utils import ensure_dir, sanitize_name, file_exists_nonempty, load_json, save_json, backoff_delays, sleep


HERE = Path(__file__).resolve().parent
PROJECT_ROOT = HERE.parents[1]
# æ•°æ®ç›®å½•ç»Ÿä¸€æ”¾åœ¨ /data ä¸‹
TMP_DIR = Path('/data/download')
STATE_PATH = Path('/data/export_state.json')
INJECT_FILE = HERE / 'tiny_alidocs_api.js'
# ä½¿ç”¨ç¯å¢ƒå˜é‡ USER_DATA_DIRï¼Œä¸ login_only.py ä¿æŒä¸€è‡´
PERSIST_DIR = Path(os.getenv("USER_DATA_DIR", "/app/persistent_context/Default"))
LOG_DIR = Path('/data/log')


def run_full_update() -> None:
    """å…¨é‡æ›´æ–°ï¼šä» 2000-01-01 å¼€å§‹ï¼ˆææ—©æ—¶é—´ï¼‰ï¼Œç­‰ä»·äºå…¨é‡ã€‚"""
    # 2000-01-01 00:00:00 çš„ Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    ts_2000_01_01 = 946684800
    log("æ‰§è¡Œå…¨é‡æ›´æ–° ...")
    run_update(ts_2000_01_01)


def run_incremental_update() -> None:
    """å¢é‡æ›´æ–°ï¼šå–"æ˜¨å¤©å‡Œæ™¨"ï¼ˆæ˜¨æ—¥ 00:00:00ï¼‰çš„ Unix ç§’å¹¶æ‰§è¡Œã€‚"""
    _setup_incremental_logging()
    yesterday_midnight = (datetime.now() - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    ts = int(yesterday_midnight.timestamp())
    log(f"æ‰§è¡Œå¢é‡æ›´æ–°ï¼Œèµ·å§‹æ—¶é—´: {yesterday_midnight.isoformat()} (ts={ts}) ...")
    run_update(ts)


def _build_targets_from_env_or_args(args_urls: Optional[List[str]] = None) -> List[str]:
    targets: List[str] = []
    if args_urls:
        targets.extend([u for u in args_urls if str(u).strip()])
    else:
        env_urls = os.getenv('TARGET_URLS', '') or ''
        if env_urls.strip():
            raw = [x for part in env_urls.split('\n') for x in part.split(',')]
            raw2 = [x for part in raw for x in part.split(';')]
            targets.extend([x.strip() for x in raw2 if x.strip()])
        else:
            single = os.getenv('TARGET_URL', '').strip()
            if single:
                targets.append(single)
    return targets


def run_update(update_time: int) -> None:
    """å•çº¿ç¨‹æ›´æ–°æµç¨‹ã€‚

    æ­¥éª¤ï¼š
    1) è§£æç›®æ ‡åˆ—è¡¨ï¼ˆTARGET_URLS/TARGET_URL æˆ–å·²å­˜åœ¨çš„å‘½ä»¤è¡Œ urlsï¼‰
    2) åˆ—è¡¨é¡µé‡‡é›† â†’ è¿‡æ»¤å‡ºéœ€è¦å¯¼å‡ºçš„ itemsï¼ˆä»… adoc/axlsï¼Œä¸” updatedTime/1000 >= update_timeï¼‰
    3) æ”¶é›†å¹¶åˆ é™¤æ—§æ–‡æ¡£ï¼ˆæ ¹æ® uuidâ†’doc_id æ˜ å°„ï¼‰
    4) é€ä¸ªå¯¼å‡ºä¸‹è½½ PDF â†’ ä¸Šä¼  RagFlow â†’ æ›´æ–°æ˜ å°„ â†’ æ¸…ç† PDFï¼›æ¯ 10 ä¸ªè§¦å‘è§£æ
    5) åŒæ­¥åˆ é™¤ï¼šæœ¬åœ°æ˜ å°„ä¸­å­˜åœ¨ä½†è¿™æ¬¡æœªå‡ºç°çš„ uuid æ‰¹é‡åˆ é™¤ RagFlow æ–‡æ¡£å¹¶åˆ é™¤æ˜ å°„
    6) ä¿å­˜æ˜ å°„å¹¶æ‰“å°ç»Ÿè®¡
    """
    # 0. åŸºæœ¬é…ç½®
    headless = os.getenv('HEADLESS', 'true').lower() == 'true'
    targets = _build_targets_from_env_or_args()
    if not targets:
        log('run_update: æœªæä¾›ä»»ä½•ç›®æ ‡ï¼ˆTARGET_URLS/TARGET_URL ä¸ºç©ºï¼‰ï¼Œé€€å‡ºã€‚')
        return

    # 1. åˆå§‹åŒ–æ˜ å°„
    id_map.ensure_initialized()
    before_keys = set(k for k, _ in list(id_map.items()))

    # 2. æµè§ˆå™¨ä¸åˆ—è¡¨é‡‡é›†
    all_selected_items: List[Dict[str, Any]] = []
    all_seen_uuid_set = set()  # æœ¬æ¬¡éå†åˆ°çš„å…¨éƒ¨æ–‡ä»¶ uuidï¼ˆéä»…å¯å¯¼å‡ºï¼‰
    with sync_playwright() as pw:
        browser = pw.chromium.launch_persistent_context(
            user_data_dir=str(PERSIST_DIR),
            headless=headless,
            args=['--disable-dev-shm-usage']
        )
        try:
            page = browser.new_page()
            inject_api(page)
            ensure_logged_in(browser, page)

            for t_idx, target_url in enumerate(targets, 1):
                try:
                    page.goto(target_url, wait_until='load')
                    inject_api(page)
                    root = resolve_root(page, target_url)
                    files = list_tree(page, root['nodeId'])
                    # è®°å½•æœ¬æ¬¡éå†åˆ°çš„å…¨éƒ¨éæ–‡ä»¶å¤¹ uuid
                    for _it in files:
                        _uid = _it.get('id')
                        if _uid:
                            all_seen_uuid_set.add(_uid)

                    def _time_ok(rec: Dict[str, Any]) -> bool:
                        try:
                            ut = rec.get('updatedTime')
                            if ut is None:
                                return False
                            return int(int(ut) / 1000) >= int(update_time)
                        except Exception:
                            return False

                    # æ”¯æŒ adoc/axls å¯¼å‡ºï¼Œä»¥åŠ docx/xlsx/pdf åŸæ ¼å¼ç›´ä¸‹
                    sel = [
                        f for f in files
                        if (
                            (f.get('extension') in ('adoc', 'axls') and f.get('docKey'))
                            or (f.get('extension') in ('docx', 'xlsx', 'pdf'))
                        ) and _time_ok(f)
                    ]
                    log(f"[{t_idx}/{len(targets)}] ç›®æ ‡ {target_url} å¯å¯¼å‡º: {len(sel)}")
                    all_selected_items.extend(sel)
                except Exception as e:
                    log(f"é‡‡é›†å¤±è´¥: {target_url}: {e}")
                    continue
        finally:
            browser.close()

    # 3. åˆ é™¤æ—§æ–‡æ¡£ï¼ˆå…ˆåˆ é™¤æ˜ å°„ä¸­è¿™äº› uuid å¯¹åº”çš„ doc_idï¼‰
    uuids_to_update = [it.get('id') for it in all_selected_items if it.get('id')]
    old_doc_ids: List[str] = []
    # ä¿å­˜æ›´æ–°å‰çš„æ˜ å°„çŠ¶æ€ï¼Œç”¨äºåç»­ç»Ÿè®¡ï¼ˆåˆ¤æ–­æ–°å¢/æ›´æ–°ï¼‰
    uuid_is_update_map: Dict[str, bool] = {}  # uuid -> True(æ›´æ–°) / False(æ–°å¢)
    for uid in uuids_to_update:
        mapping_value = id_map.get(uid)  # æœŸæœ›ä¸º dict
        doc_id = mapping_value.get('ragflow_doc_id') if isinstance(mapping_value, dict) else None
        file_url = mapping_value.get('ding_doc_url') if isinstance(mapping_value, dict) else None
        if not file_url:
            file_url = f"https://alidocs.dingtalk.com/i/nodes/{uid}"
        uuid_is_update_map[uid] = doc_id is not None  # å¦‚æœå·²æœ‰æ˜ å°„ï¼Œåˆ™ä¸ºæ›´æ–°
        if doc_id:
            old_doc_ids.append(doc_id)
    
    # ç»Ÿè®¡ï¼šåˆ é™¤çš„æ—§æ–‡æ¡£æ•°é‡
    deleted_before_update_count = len(old_doc_ids)
    if old_doc_ids:
        ok = ragflow_api.delete_documents(old_doc_ids)
        log(f"åˆ é™¤æ—§æ–‡æ¡£: {len(old_doc_ids)} -> {'OK' if ok else 'FAIL'}")

    # 4. å¯¼å‡º/ä¸Šä¼ /æ˜ å°„/è§£æï¼ˆå•çº¿ç¨‹ï¼‰
    uploaded_ids: List[str] = []
    parsed_buffer: List[str] = []
    state = load_json(STATE_PATH, default={'completed': {}})
    
    # ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
    stats = {
        'success_count': 0,
        'fail_count': 0,
        'success_items': [],
        'fail_items': [],
        'by_type': {},  # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        'by_operation': {'new': 0, 'update': 0},  # æ–°å¢/æ›´æ–°
        'ragflow_success': 0,
    }

    with sync_playwright() as pw:
        browser = pw.chromium.launch_persistent_context(
            user_data_dir=str(PERSIST_DIR),
            headless=headless,
            args=['--disable-dev-shm-usage']
        )
        try:
            page = browser.new_page()
            inject_api(page)
            ensure_logged_in(browser, page)

            for idx, item in enumerate(all_selected_items, 1):
                item_uuid = item.get('id', '')
                item_name = item.get('name', '')
                item_ext = (item.get('extension') or '').lower()
                # ä½¿ç”¨åˆ é™¤å‰ä¿å­˜çš„æ˜ å°„çŠ¶æ€åˆ¤æ–­æ˜¯æ–°å¢è¿˜æ˜¯æ›´æ–°
                is_update = uuid_is_update_map.get(item_uuid, False)
                
                try:
                    export_and_download(page, browser, item, state, min_unix_ts=update_time)
                    save_json(STATE_PATH, state)
                    # ä¸Šä¼ 
                    out_path = out_path_for(item)
                    if out_path.exists():
                        doc_id = ragflow_api.upload_document(out_path)
                        # ä½¿ç”¨æ–°æ ¼å¼ä¿å­˜æ˜ å°„ï¼ˆåŒ…å« ragflow_doc_id å’Œ ding_doc_urlï¼‰
                        ding_doc_url = f"https://alidocs.dingtalk.com/i/nodes/{item['id']}"
                        id_map.put_ragflow_mapping(item['id'], doc_id, auto_save=False)

                        # æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®ï¼Œè®¾ç½® url å­—æ®µ
                        try:
                            ragflow_api.update_document_metadata(
                                doc_id,
                                meta_fields={"url": ding_doc_url}
                            )
                            log(f"RagFlow å…ƒæ•°æ®æ›´æ–°æˆåŠŸ: {item['id']} -> {ding_doc_url}")
                        except Exception as e:
                            log(f"RagFlow å…ƒæ•°æ®æ›´æ–°å¤±è´¥: {item['id']} -> {e}")

                        stats['ragflow_success'] += 1

                        uploaded_ids.append(doc_id)
                        parsed_buffer.append(doc_id)
                        
                        # ç»Ÿè®¡æˆåŠŸ
                        stats['success_count'] += 1
                        stats['success_items'].append({
                            'name': item_name,
                            'uuid': item_uuid,
                            'type': item_ext,
                            'doc_id': doc_id
                        })
                        stats['by_type'][item_ext] = stats['by_type'].get(item_ext, 0) + 1
                        if is_update:
                            stats['by_operation']['update'] += 1
                        else:
                            stats['by_operation']['new'] += 1
                        
                        # æ¸…ç† PDF
                        try:
                            out_path.unlink(missing_ok=True)
                        except Exception:
                            pass
                        # è§¦å‘è§£æï¼ˆæ¯ 10 ä¸ªï¼‰
                        if len(parsed_buffer) >= 10:
                            try:
                                ragflow_api.parse_documents(parsed_buffer)
                                log(f"[è§£ææˆåŠŸ] æ‰¹é‡ {len(parsed_buffer)} ä¸ªæ–‡æ¡£")
                            except Exception as e:
                                log(f"[è§£æå¤±è´¥] æ‰¹é‡ {len(parsed_buffer)} ä¸ªæ–‡æ¡£: {e}")
                            parsed_buffer.clear()
                    else:
                        log(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {out_path}")
                        stats['fail_count'] += 1
                        stats['fail_items'].append({
                            'name': item_name,
                            'uuid': item_uuid,
                            'type': item_ext,
                            'error': 'æ–‡ä»¶ä¸å­˜åœ¨'
                        })
                except Exception as e:
                    log(f"å¤„ç†å¤±è´¥: {item.get('id')} -> {e}")
                    stats['fail_count'] += 1
                    stats['fail_items'].append({
                        'name': item_name,
                        'uuid': item_uuid,
                        'type': item_ext,
                        'error': str(e)
                    })
        finally:
            browser.close()

    # å¤„ç†å‰©ä½™ä¸è¶³10ä¸ªçš„æ–‡æ¡£
    if parsed_buffer:
        try:
            ragflow_api.parse_documents(parsed_buffer)
            log(f"[è§£ææˆåŠŸ] å‰©ä½™ {len(parsed_buffer)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            log(f"[è§£æå¤±è´¥] å‰©ä½™ {len(parsed_buffer)} ä¸ªæ–‡æ¡£: {e}")
        parsed_buffer.clear()

    # 5. åŒæ­¥åˆ é™¤ï¼šæœ¬åœ°æ˜ å°„ä¸­å­˜åœ¨ä½†è¿™æ¬¡æ²¡å‡ºç°çš„ uuidï¼ˆä¸"æœ¬æ¬¡å…¨éƒ¨éå†åˆ°çš„ uuid"å¯¹æ¯”ï¼‰
    current_uuid_set = set(all_seen_uuid_set)
    stale_doc_ids: List[str] = []
    stale_uuids: List[str] = []  # è®°å½•éœ€è¦åˆ é™¤çš„ uuid
    for uid, value in list(id_map.items()):
        if uid not in current_uuid_set:
            doc_id = id_map.get_ragflow_doc_id(uid)
            file_url = value.get('ding_doc_url') if isinstance(value, dict) else None
            if not file_url:
                file_url = f"https://alidocs.dingtalk.com/i/nodes/{uid}"
            if doc_id:
                stale_doc_ids.append(doc_id)
            stale_uuids.append(uid)
    
    # ç»Ÿè®¡ï¼šåŒæ­¥åˆ é™¤çš„æ•°é‡
    sync_deleted_count = len(stale_doc_ids)
    if stale_doc_ids:
        ok = ragflow_api.delete_documents(stale_doc_ids)
        log(f"åŒæ­¥åˆ é™¤è¿œç«¯: {len(stale_doc_ids)} -> {'OK' if ok else 'FAIL'}")
        if ok:
            # åˆ é™¤æœ¬åœ°æ˜ å°„
            for uid in stale_uuids:
                id_map.delete(uid, auto_save=False)

    # 6. ä¿å­˜æ˜ å°„
    id_map.save()

    # 7. ç»Ÿè®¡
    after_keys = set(k for k, _ in list(id_map.items()))
    total_deleted = deleted_before_update_count + sync_deleted_count
    
    # ç”Ÿæˆå¹¶å‘é€è¯¦ç»†çš„ç»Ÿè®¡æ¶ˆæ¯
    _send_statistics_notification(
        update_time=update_time,
        targets=targets,
        stats=stats,
        before_keys_count=len(before_keys),
        after_keys_count=len(after_keys),
        total_selected=len(all_selected_items),
        deleted_before_update=deleted_before_update_count,
        sync_deleted=sync_deleted_count,
        total_deleted=total_deleted,
        uploaded_count=len(uploaded_ids),
        parsed_count=len(uploaded_ids)  # æ‰€æœ‰ä¸Šä¼ çš„éƒ½ä¼šè§£æ
    )
    
    log(f"run_update å®Œæˆï¼šé€‰ä¸­ {len(all_selected_items)}ï¼Œä¸Šä¼  {len(uploaded_ids)}ï¼Œæ˜ å°„è®¡æ•° {len(after_keys)}ï¼ˆåŸ {len(before_keys)}ï¼‰")

def _send_statistics_notification(
    update_time: int,
    targets: List[str],
    stats: Dict[str, Any],
    before_keys_count: int,
    after_keys_count: int,
    total_selected: int,
    deleted_before_update: int,
    sync_deleted: int,
    total_deleted: int,
    uploaded_count: int,
    parsed_count: int
) -> None:
    """ç”Ÿæˆå¹¶å‘é€è¯¦ç»†çš„ç»Ÿè®¡æ¶ˆæ¯åˆ°é’‰é’‰"""
    if not ROBOT_ACCESS_TOKEN or not ROBOT_SECRET:
        log("æœªé…ç½®é’‰é’‰æœºå™¨äººï¼Œè·³è¿‡ç»Ÿè®¡é€šçŸ¥")
        return
    
    try:
        # æ—¥æœŸå’Œæ—¶é—´èŒƒå›´
        now = datetime.now()
        date_str = now.strftime("%Y-%m-%d %H:%M:%S")
        update_datetime = datetime.fromtimestamp(update_time)
        
        # æ„å»º Markdown æ¶ˆæ¯
        lines = []
        lines.append(f"#### ğŸ“Š é’‰é’‰æ–‡æ¡£åŒæ­¥ç»Ÿè®¡æŠ¥å‘Š")
        lines.append("")
        lines.append(f"**æ‰§è¡Œæ—¶é—´**: {date_str}")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # ç›®æ ‡ç»Ÿè®¡
        lines.append("##### ğŸ“ ç›®æ ‡ç›®å½•")
        for idx, target in enumerate(targets, 1):
            # æˆªæ–­è¿‡é•¿çš„ URL
            target_display = target if len(target) <= 80 else target[:77] + "..."
            lines.append(f"{idx}. `{target_display}`")
        lines.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        lines.append("##### ğŸ“ˆ ä¸Šä¼ ç»Ÿè®¡")
        lines.append(f"- **æˆåŠŸå¤„ç†**: {stats['success_count']} ä¸ª")
        lines.append(f"- **å¤„ç†å¤±è´¥**: {stats['fail_count']} ä¸ª")
        lines.append(f"- **RagFlow ä»»åŠ¡**: {stats['ragflow_success']} ä¸ª")
        lines.append("")
        
        # æ“ä½œç±»å‹ç»Ÿè®¡
        lines.append("##### ğŸ”„ æ“ä½œç±»å‹")
        lines.append(f"- **æ–°å¢æ–‡æ¡£**: {stats['by_operation']['new']} ä¸ª")
        lines.append(f"- **æ›´æ–°æ–‡æ¡£**: {stats['by_operation']['update']} ä¸ª")
        lines.append(f"- **åˆ é™¤æ–‡æ¡£**: {sync_deleted} ä¸ª")
        lines.append("")
        
        markdown_text = "\n".join(lines)
        
        # å‘é€é’‰é’‰æ¶ˆæ¯
        send_dingtalk_markdown(
            ROBOT_ACCESS_TOKEN,
            ROBOT_SECRET,
            "é’‰é’‰æ–‡æ¡£åŒæ­¥ç»Ÿè®¡æŠ¥å‘Š",
            markdown_text
        )
        
        log("ç»Ÿè®¡æ¶ˆæ¯å·²å‘é€åˆ°é’‰é’‰")
    except Exception as e:
        log(f"å‘é€ç»Ÿè®¡æ¶ˆæ¯å¤±è´¥: {e}")


def log(message: str) -> None:
    try:
        # print(f"[PRC] {message}")
        try:
            logging.info(message)
        except Exception:
            pass
    except Exception:
        pass

def _mask_token(value: str) -> str:
    try:
        if not value:
            return "<empty>"
        v = str(value)
        if len(v) <= 8:
            return "*" * max(1, len(v) - 2) + v[-2:]
        return v[:4] + "..." + v[-4:]
    except Exception:
        return "<masked>"


def inject_api(page: Page) -> None:
    log("æ³¨å…¥ tiny_alidocs_api.js ...")
    content = INJECT_FILE.read_text(encoding='utf-8')
    # æ­£ç¡®å‚æ•°åä¸º scriptï¼ˆPython Playwrightï¼‰
    page.add_init_script(script=content)
    # ç¡®ä¿åœ¨å½“å‰å·²åŠ è½½é¡µé¢ä¹Ÿå¯ç«‹å³ç”Ÿæ•ˆ
    try:
        page.evaluate("() => { if (!window.alidocs) {" + content + "} }")
        log("æ³¨å…¥å®Œæˆå¹¶å·²åœ¨å½“å‰é¡µç”Ÿæ•ˆ")
    except Exception:
        log("æ³¨å…¥å®Œæˆï¼ˆå½“å‰é¡µç”Ÿæ•ˆæ ¡éªŒç•¥è¿‡ï¼‰")


def _setup_incremental_logging() -> None:
    try:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        fname = f"incremental_{datetime.now().strftime('%Y%m%d')}.log"
        fpath = LOG_DIR / fname
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        # æ¸…ç†å·²æœ‰å¤„ç†å™¨ï¼Œé¿å…é‡å¤å†™å…¥
        if not any(isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', '').endswith(str(fpath)) for h in logger.handlers):
            logger.handlers = []
            fh = logging.FileHandler(str(fpath), encoding='utf-8')
            sh = logging.StreamHandler(sys.stdout)
            fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            fh.setFormatter(fmt)
            sh.setFormatter(fmt)
            logger.addHandler(fh)
            logger.addHandler(sh)
        # æ¸…ç†30å¤©å‰çš„æ—¥å¿—
        try:
            cutoff = time.time() - 30 * 24 * 3600
            for entry in LOG_DIR.iterdir():
                try:
                    if entry.is_file() and entry.name.startswith('incremental_') and entry.suffix == '.log':
                        if entry.stat().st_mtime < cutoff:
                            entry.unlink(missing_ok=True)
                except Exception:
                    pass
        except Exception:
            pass
    except Exception:
        pass


# ======== ç™»å½•ä¸æœºå™¨äººé€šçŸ¥ï¼šå¤ç”¨ simple_test.py æ€è·¯ï¼ˆåŒæ­¥ç‰ˆï¼‰ ========
DING_LOGIN_URL = os.getenv(
    "DING_LOGIN_URL",
    "https://login.dingtalk.com/oauth2/challenge.htm?redirect_uri=https%3A%2F%2Falidocs.dingtalk.com%2Fi%2F%3Fspm%3Da2q1e.24441682.0.0.2c8b252137UE4J&response_type=none&client_id=dingoaxhhpeq7is6j1sapz&scope=openid",
)
ROBOT_ACCESS_TOKEN = os.getenv("DING_ROBOT_ACCESS_TOKEN", "c47714769baf275fa072ad78169f0c9bfc96612e74ec08eee0b111f1804eea76")
ROBOT_SECRET = os.getenv("DING_ROBOT_SECRET", "SEC7e1604e3cf35b1a4543f8acf1750dae2946e3ee48f601d7672489734be0c7e98")
PICUI_TOKEN = os.getenv("PICUI_TOKEN", "1795|GdzkBaU9wreWyuhYls9Y06WUQZ3mGB7b1aQrDp7e")
PICUI_API = os.getenv("PICUI_API", "https://picui.cn/api/v1")
AT_MOBILES_ENV = os.getenv("AT_MOBILES", "")
AT_MOBILES_LIST = [m.strip() for m in AT_MOBILES_ENV.split(',') if m.strip()]

AT_USER_IDS_ENV = os.getenv("AT_USER_IDS", "")
AT_USER_IDS_LIST = [u.strip() for u in AT_USER_IDS_ENV.split(',') if u.strip()]
TRIGGER_BASE_URL = os.getenv("TRIGGER_BASE_URL", "http://localhost:8999")

QR_LOGIN_SELECTOR = 'text="æ‰«ç ç™»å½•"'
TARGET_SELECTORS = [
    'div.module-qrcode-op-line div.base-comp-check-box-rememberme-box.dingtalk-login-iconfont.dingtalk-login-icon-checkbox-undone',
    'div.module-qrcode-op-line-with-open-passkey div.base-comp-check-box-rememberme-box.dingtalk-login-iconfont.dingtalk-login-icon-checkbox-undone',
    'div.module-qrcode-op-item div.base-comp-check-box-rememberme-box.dingtalk-login-iconfont.dingtalk-login-icon-checkbox-undone',
]


def _post_with_log(url, **kwargs):
    try:
        resp = requests.post(url, timeout=kwargs.pop("timeout", 20), **kwargs)
        return resp
    except Exception as e:
        raise RuntimeError(f"POST {url} failed: {e}")


def _put_with_log(url, data, timeout=30):
    try:
        resp = requests.put(url, data=data, timeout=timeout)
        return resp
    except Exception as e:
        raise RuntimeError(f"PUT {url} failed: {e}")


def send_custom_robot_group_message(access_token: str, secret: str, msg: str, at_user_ids=None, at_mobiles=None, is_at_all: bool = False):
    """å‘é€é’‰é’‰è‡ªå®šä¹‰æœºå™¨äººç¾¤æ¶ˆæ¯ï¼ˆActionCardï¼‰ã€‚
    å‚è€ƒç”¨æˆ·æä¾›çš„ç¤ºä¾‹å®ç°ã€‚
    """
    timestamp = str(round(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    hmac_code = hmac.new(secret.encode('utf-8'), string_to_sign.encode('utf-8'), digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))

    url = f"https://oapi.dingtalk.com/robot/send?access_token={access_token}&timestamp={timestamp}&sign={sign}"

    body = {
        "at": {
            "isAtAll": str(is_at_all).lower(),
            "atUserIds": at_user_ids or AT_USER_IDS_LIST,
            "atMobiles": at_mobiles or AT_MOBILES_LIST,
        },
        "msgtype": "actionCard",
        "actionCard": {
            "title": "ç™»å½•è¿‡æœŸ",
            "text": msg or "ç™»å½•è¿‡æœŸï¼Œé’‰é’‰æ–‡æ¡£è‡ªåŠ¨åŒ–è„šæœ¬è¿è¡Œå¤±è´¥! \n\nè¯·å¯¹åº”è´Ÿè´£äººé‡æ–°æ‰«ç ç™»å½•ã€‚",
            "btnOrientation": "0",
            "btns": [
                {
                    "title": "æ‰«ç ç™»å½•",
                    "actionURL": f"{TRIGGER_BASE_URL}/start-login?token=abc"
                }
            ]
        }
    }
    headers = {'Content-Type': 'application/json'}
    try:
        resp = requests.post(url, json=body, headers=headers, timeout=15)
        _ = resp.text
    except Exception:
        pass


def send_dingtalk_markdown(access_token: str, secret: str, title: str, text: str, at_mobiles=None):
    """å‘é€é’‰é’‰ Markdown æ¶ˆæ¯"""
    timestamp = str(round(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    hmac_code = hmac.new(secret.encode('utf-8'), string_to_sign.encode('utf-8'), digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    
    url = f"https://oapi.dingtalk.com/robot/send?access_token={access_token}&timestamp={timestamp}&sign={sign}"
    
    body = {
        "msgtype": "markdown",
        "markdown": {
            "title": title,
            "text": text
        },
        "at": {
            "isAtAll": False,
            "atMobiles": at_mobiles or AT_MOBILES_LIST,
        }
    }
    headers = {'Content-Type': 'application/json'}
    try:
        resp = requests.post(url, json=body, headers=headers, timeout=15)
        _ = resp.text
    except Exception as e:
        log(f"å‘é€é’‰é’‰æ¶ˆæ¯å¤±è´¥: {e}")


def upload_image_return_url(image_path: str) -> str:
    headers = {"Accept": "application/json"}
    if PICUI_TOKEN:
        headers["Authorization"] = f"Bearer {PICUI_TOKEN}"
    try:
        with open(image_path, "rb") as f:
            resp = _post_with_log(
                f"{PICUI_API}/upload",
                headers=headers,
                files={"file": (os.path.basename(image_path), f, "image/png")},
                timeout=25,
            )
        if resp.headers.get("Content-Type", "").startswith("application/json"):
            data = resp.json()
        else:
            data = {}
        if data.get("status") and data.get("data"):
            url = data["data"].get("url") or (data["data"].get("links", {}) or {}).get("url")
            if url:
                return url
    except Exception:
        pass
    # é€€åŒ– 0x0.st
    try:
        with open(image_path, "rb") as f:
            resp = requests.post("https://0x0.st", files={"file": f}, timeout=15)
        if resp.ok and resp.text.strip().startswith("http"):
            return resp.text.strip()
    except Exception:
        pass
    raise RuntimeError("æ— æ³•ä¸Šä¼ å›¾ç‰‡åˆ°å…¬å…±å›¾åºŠ")


def sign_robot_request(secret: str):
    timestamp = str(round(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return timestamp, sign


def send_dingtalk_markdown_image(access_token: str, secret: str, title: str, image_url: str, extra_text: str = "", at_mobiles=None):
    ts, sign = sign_robot_request(secret)
    url = f"https://oapi.dingtalk.com/robot/send?access_token={access_token}&timestamp={ts}&sign={sign}"
    text_lines = []
    if extra_text:
        text_lines.append(extra_text)
    text_lines.append(f"![screenshot]({image_url})")
    payload = {
        "msgtype": "markdown",
        "markdown": {"title": title, "text": "\n\n".join(text_lines)},
        "at": {"isAtAll": False, "atMobiles": at_mobiles or []},
    }
    resp = requests.post(url, json=payload, headers={"Content-Type": "application/json"}, timeout=15)
    try:
        _ = resp.json()
    except Exception:
        pass


def ensure_logged_in(context: BrowserContext, page: Page):
    """ä»…æ£€æŸ¥æ˜¯å¦å·²ç™»å½•ã€‚
    - å·²ç™»å½•ï¼šç»§ç»­
    - æœªç™»å½•ï¼šå‘é€ ActionCard å¹¶ç»ˆæ­¢
    ä¸å†åœ¨ main.py ä¸­æ‰§è¡Œä»»ä½•ç™»å½•ç›¸å…³çš„ UI æ“ä½œæˆ–æˆªå›¾ã€‚
    """
    log("ç™»å½•æ£€æŸ¥ï¼šè®¿é—®æ¡Œé¢é¡µ")
    try:
        page.goto("https://alidocs.dingtalk.com/i/desktop", wait_until="domcontentloaded", timeout=10000)
        if "alidocs.dingtalk.com/i/desktop" in page.url:
            log("å·²ç™»å½•")
            return
        log("æœªç™»å½•ï¼Œå‘é€æ‰«ç ç™»å½•é€šçŸ¥å¹¶ç»ˆæ­¢")
        if ROBOT_ACCESS_TOKEN and ROBOT_SECRET:
            try:
                send_custom_robot_group_message(
                    ROBOT_ACCESS_TOKEN,
                    ROBOT_SECRET,
                    "ç™»å½•è¿‡æœŸï¼Œé’‰é’‰æ–‡æ¡£è‡ªåŠ¨åŒ–è„šæœ¬è¿è¡Œå¤±è´¥! \n\nè¯·å¯¹åº”è´Ÿè´£äººé‡æ–°æ‰«ç ç™»å½•ã€‚",
                    at_user_ids=None,
                    at_mobiles=AT_MOBILES_LIST,
                    is_at_all=False,
                )
            except Exception as e:
                log("å‘é€ ActionCard å¤±è´¥: " + str(e))
        raise SystemExit(2)
    except Exception as e:
        log(f"æ¡Œé¢é¡µè®¿é—®å¤±è´¥: {e}")
        # å¤±è´¥æ—¶æŒ‰æœªç™»å½•å¤„ç†
        if ROBOT_ACCESS_TOKEN and ROBOT_SECRET:
            try:
                send_custom_robot_group_message(
                    ROBOT_ACCESS_TOKEN,
                    ROBOT_SECRET,
                    "æ— æ³•è®¿é—®æ¡Œé¢é¡µï¼Œå¯èƒ½æœªç™»å½•æˆ–ç½‘ç»œå¼‚å¸¸ã€‚è¯·æ‰«ç ç™»å½•åé‡è¯•ã€‚",
                    at_user_ids=None,
                    at_mobiles=AT_MOBILES_LIST,
                    is_at_all=False,
                )
            except Exception as e2:
                log("å‘é€ ActionCard å¤±è´¥: " + str(e2))
        raise SystemExit(2)


def call_api(page: Page, fn: str, *args):
    # Playwright Python evaluate åªæ¥å—ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œè¿™é‡Œé€šè¿‡å¯¹è±¡æ‰“åŒ…ä¼ å…¥
    return page.evaluate(
        "(params) => window.alidocs[params.fn](...(params.args || []))",
        {"fn": fn, "args": list(args)}
    )


def resolve_root(page: Page, url_or_id: str) -> Dict[str, Any]:
    log(f"è§£ææ ¹èŠ‚ç‚¹: {url_or_id}")
    r = call_api(page, 'resolveNode', url_or_id)
    if not r.get('ok'):
        raise RuntimeError(f"resolveNode failed: {r}")
    data = r['data']
    log(f"æ ¹èŠ‚ç‚¹è§£ææˆåŠŸ: nodeId={data.get('nodeId')} type={data.get('type')}")
    return data


def list_tree(page: Page, root_id: str) -> List[Dict[str, Any]]:
    results: List[Dict[str, Any]] = []
    stack: List[Dict[str, Any]] = [{ 'id': root_id, 'rel': [] }]

    while stack:
        cur = stack.pop()
        parent_id = cur['id']
        # åˆ—ä¸¾å½“å‰çˆ¶èŠ‚ç‚¹
        # log(f"åˆ—ä¸¾: rel={'/'.join(cur['rel']) or '/'}")
        cursor: Optional[str] = None
        seen_cursors = set()
        page_count = 0
        while True:
            r = call_api(page, 'listChildren', parent_id, cursor)
            if not r.get('ok'):
                raise RuntimeError(f"listChildren failed for {parent_id}: {r}")
            data = r['data']
            items = data.get('items') or []
            # å¯æŒ‰éœ€æ‰“å¼€åˆ†é¡µæ—¥å¿—
            for it in items:
                typ = it.get('type')
                name = sanitize_name(it.get('name') or '')
                const_has_children = bool(it.get('hasChildren'))
                # æ”¯æŒâ€œæ–‡ä»¶ä¹Ÿå¯èƒ½æœ‰ä¸‹çº§â€ï¼šæœ‰ä¸‹çº§å°±å…¥æ ˆç»§ç»­éå†
                if typ == 'folder' or const_has_children:
                    stack.append({ 'id': it['id'], 'rel': cur['rel'] + [name] })
                # éæ–‡ä»¶å¤¹ï¼ˆåŒ…æ‹¬æœ‰ä¸‹çº§çš„æ–‡ä»¶æœ¬èº«ï¼‰éƒ½åº”è®¡å…¥ç»“æœï¼Œä¾›å¯¼å‡º
                if typ != 'folder':
                    results.append({
                        'id': it['id'],
                        'type': typ,
                        'name': name,
                        'rel': cur['rel'],
                        'contentType': it.get('contentType'),
                        'docKey': it.get('docKey'),
                        'dentryKey': it.get('dentryKey'),
                        'extension': it.get('extension'),
                        'updatedTime': it.get('updatedTime'),
                        'uuid': it.get('id')
                    })
            next_cursor = data.get('nextCursor')
            # ç»ˆæ­¢æ¡ä»¶ï¼šæ—  nextã€items ä¸ºç©ºã€æˆ– cursor æœªæ¨è¿›ã€æˆ–é¡µæ•°è¿‡å¤š
            if not next_cursor:
                break
            if len(items) == 0:
                break
            if next_cursor == cursor or next_cursor in seen_cursors:
                break
            cursor = next_cursor
            seen_cursors.add(cursor)
            page_count += 1
            if page_count >= 200:
                break
    log(f"æ€»æ–‡ä»¶æ•°: {len(results)}")
    # æ–‡ä»¶æ¸…å•ï¼ˆå«ç›¸å¯¹è·¯å¾„ï¼‰
    try:
        for idx, r in enumerate(results, 1):
            relpath = "/".join([*r['rel'], r['name']]) if r.get('rel') else r['name']
            ext = r.get('extension') or (r.get('contentType') or '')
            if idx <= 50:
                log(f"  [{idx}] {relpath}  type={r.get('type')} ext={ext} updatedTime={r.get('updatedTime')} uuid={r.get('uuid')}")
            elif idx == 51:
                log("  ... çœç•¥åç»­æ¡ç›® ...")
    except Exception:
        pass
    # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
    dist: Dict[str, int] = {}
    for r in results:
        dist[r['type']] = dist.get(r['type'], 0) + 1
    log("ç±»å‹åˆ†å¸ƒ: " + ", ".join([f"{k}={v}" for k,v in dist.items()]))
    return results


def ext_for_item(item: Dict[str, Any]) -> Optional[str]:
    """å†³å®šå¯¼å‡º/ä¸‹è½½çš„ç›®æ ‡æ‰©å±•åï¼š
    - adoc â†’ pdfï¼ˆå¯¼å‡ºï¼‰
    - axls â†’ xlsxï¼ˆå¯¼å‡ºï¼‰
    - åŸå§‹ä¸Šä¼ çš„ docx/xlsx/pdf â†’ åŸæ ¼å¼ç›´ä¸‹
    å…¶ä»–è¿”å› Noneï¼ˆè·³è¿‡ï¼‰ã€‚
    """
    ext = (item.get('extension') or '').lower()
    if ext in ('docx', 'xlsx', 'pdf'):
        return ext
    itype = item.get('type')
    if itype == 'doc':
        return 'pdf'
    if itype == 'sheet':
        return 'xlsx'
    return None


def out_path_for(item: Dict[str, Any]) -> Path:
    """è¿”å›ä¸‹è½½æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ï¼š
    - æ‰å¹³ç›®å½•ï¼šæ‰€æœ‰æ–‡ä»¶ç›´æ¥ä¿å­˜åœ¨ TMP_DIR ä¸‹
    - æ–‡ä»¶åä½¿ç”¨ nameï¼ˆæ¸…ç†åçš„åç§°ï¼‰ï¼Œè€Œä¸æ˜¯ uuid
    """
    ext = ext_for_item(item)
    ensure_dir(TMP_DIR)
    # ä½¿ç”¨æ¸…ç†åçš„åç§°ä½œä¸ºæ–‡ä»¶å
    name = sanitize_name(item.get('name', 'untitled'), max_len=200)
    filename = f"{name}.{ext}"
    return TMP_DIR / filename


def export_and_download(page: Page, ctx: BrowserContext, item: Dict[str, Any], state: Dict[str, Any], min_unix_ts: Optional[int] = None):
    ext = ext_for_item(item)
    if not ext:
        return
    out_path = out_path_for(item)
    if file_exists_nonempty(out_path):
        # å·²å­˜åœ¨åˆ™åˆ é™¤ï¼Œå¼ºåˆ¶è¦†ç›–
        try:
            out_path.unlink(missing_ok=True)
            log(f"å·²å­˜åœ¨ï¼Œåˆ é™¤å¹¶è¦†ç›–: {out_path}")
        except Exception as e:
            log(f"åˆ é™¤å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥: {out_path}, {e}")

    download_url: Optional[str] = None
    # åˆ†æ”¯ï¼šåŸå§‹æ–‡ä»¶ docx/xlsx/pdf ç›´é“¾ä¸‹è½½
    if (item.get('extension') or '').lower() in ('docx', 'xlsx', 'pdf'):
        log(f"ç›´é“¾ä¸‹è½½åŸå§‹æ–‡ä»¶: {item['name']} -> {ext}")
        last_err = None
        for d in backoff_delays():
            try:
                r = call_api(page, 'downloadDocument', item['id'])
            except Exception as e:
                last_err = str(e)
                log(f"  è·å–ç›´é“¾å¼‚å¸¸: {last_err}ï¼Œé‡è¯•{d}s")
                sleep(d)
                continue
            if r.get('ok') and r['data'].get('url'):
                download_url = r['data']['url']
                break
            last_err = r.get('error') or 'unknown error'
            log(f"  è·å–ç›´é“¾å¤±è´¥: {last_err}ï¼Œé‡è¯•{d}s")
            sleep(d)
        if not download_url:
            raise RuntimeError(f"downloadDocument failed: {item['name']}: {last_err}")
    else:
        # åˆ†æ”¯ï¼šadoc/axls å¯¼å‡ºä»»åŠ¡
        log(f"åˆ›å»ºå¯¼å‡ºä»»åŠ¡: {item['name']} -> {ext}")
        last_err = None
        for d in backoff_delays():
            try:
                r = call_api(page, 'createExportTask', item['id'], ext)
            except Exception as e:
                last_err = str(e)
                log(f"  åˆ›å»ºå¼‚å¸¸: {last_err}ï¼Œé‡è¯•{d}s")
                sleep(d)
                continue
            if r.get('ok') and r['data'].get('taskId'):
                task_id = r['data']['taskId']
                log(f"  ä»»åŠ¡å·²åˆ›å»º: {task_id}")
                break
            last_err = r.get('error') or 'unknown error'
            log(f"  åˆ›å»ºå¤±è´¥: {last_err}ï¼Œé‡è¯•{d}s")
            sleep(d)
        else:
            raise RuntimeError(f"createExportTask failed: {item['name']}: {last_err}")

        # è½®è¯¢ä»»åŠ¡
        for i in range(30):
            r = call_api(page, 'getExportTask', task_id)
            if not r.get('ok'):
                if (i + 1) % 5 == 0:
                    log(f"  è½®è¯¢å¤±è´¥({i+1})")
                sleep(2)
                continue
            data = r['data']
            st = str(data.get('status'))
            if st.lower() == 'success' and data.get('downloadUrl'):
                download_url = data['downloadUrl']
                log("  å¯¼å‡ºå®Œæˆ")
                break
            if st.lower() == 'failed':
                raise RuntimeError(f"export failed: {data}")
            sleep(2)
        if not download_url:
            raise RuntimeError("export timeout")

    # ä¸‹è½½ï¼ˆå¸¦ Cookieï¼‰
    ensure_dir(out_path.parent)
    attempt = 0
    for d in backoff_delays():
        attempt += 1
        resp = ctx.request.get(download_url)
        if resp.ok:
            with open(out_path, 'wb') as f:
                f.write(resp.body())
            if file_exists_nonempty(out_path):
                state.setdefault('completed', {})[item['id']] = { 'file': str(out_path) }
                log(f"ä¸‹è½½å®Œæˆ: {item['name']}")
                return
        if attempt >= 3:
            log("ä¸‹è½½å¤±è´¥ï¼Œé‡è¯•ä¸­")
        sleep(d)
    raise RuntimeError(f"download failed: {download_url}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--url', required=False, action='append', help='ç›®å½•é“¾æ¥æˆ–èŠ‚ç‚¹IDï¼Œå¯é‡å¤ä¼ ï¼›å¦‚æœªæä¾›å°†ä»ç¯å¢ƒå˜é‡ TARGET_URLS/TARGET_URL è¯»å–')
    ap.add_argument('--headless', required=False, choices=['true','false'], default=os.getenv('HEADLESS', 'true').lower())
    ap.add_argument('--min_ts', required=False, type=int, help='åªå¯¼å‡ºæ›´æ–°æ—¶é—´ï¼ˆç§’ï¼‰>= è¯¥å€¼çš„æ–‡æ¡£ï¼›ä¹Ÿå¯ç”¨ç¯å¢ƒå˜é‡ MIN_TS æŒ‡å®š')
    ap.add_argument('--mode', required=False, choices=['full','incremental'], help='è¿è¡Œæ¨¡å¼ï¼šfullï¼ˆå…¨é‡ï¼‰æˆ– incrementalï¼ˆå¢é‡ï¼‰')
    args = ap.parse_args()

    # è‹¥æŒ‡å®šè¿è¡Œæ¨¡å¼ï¼Œç›´æ¥æ‰§è¡Œå¹¶è¿”å›
    if args.mode == 'full':
        run_full_update()
        return
    if args.mode == 'incremental':
        run_incremental_update()
        return

    # æ±‡æ€»ç›®æ ‡åˆ—è¡¨ï¼šä¼˜å…ˆå‘½ä»¤è¡Œ --urlï¼ˆå¯å¤šæ¬¡ï¼‰ï¼Œå¦åˆ™è¯»å–ç¯å¢ƒå˜é‡
    target_list: List[str] = []
    if args.url:
        target_list.extend([u for u in args.url if str(u).strip()])
    else:
        env_urls = os.getenv('TARGET_URLS', '') or ''
        if env_urls.strip():
            # æ”¯æŒé€—å·ã€åˆ†å·ã€æ¢è¡Œåˆ†éš”
            raw = [x for part in env_urls.split('\n') for x in part.split(',')]
            raw2 = [x for part in raw for x in part.split(';')]
            target_list.extend([x.strip() for x in raw2 if x.strip()])
        else:
            single = os.getenv('TARGET_URL', '').strip()
            if single:
                target_list.append(single)

    if not target_list:
        log('æœªæä¾›ä»»ä½•ç›®æ ‡ï¼š--url / TARGET_URLS / TARGET_URL å‡ä¸ºç©ºï¼Œé€€å‡ºã€‚')
        raise SystemExit(2)

    headless = str(args.headless).lower() == 'true'
    env_min_ts = os.getenv('MIN_TS')
    min_ts: Optional[int] = None
    if args.min_ts is not None:
        min_ts = int(args.min_ts)
    elif env_min_ts is not None and str(env_min_ts).strip() != '':
        try:
            min_ts = int(str(env_min_ts).strip())
        except Exception:
            min_ts = None
    log(f"å¯åŠ¨: headless={headless}")
    ensure_dir(TMP_DIR)
    ensure_dir(PERSIST_DIR)
    log(f"è¾“å‡º: {TMP_DIR}")

    state = load_json(STATE_PATH, default={ 'completed': {} })

    with sync_playwright() as pw:
        browser = pw.chromium.launch_persistent_context(
            user_data_dir=str(PERSIST_DIR),
            headless=headless,
            args=['--disable-dev-shm-usage']
        )
        try:
            page = browser.new_page()
            inject_api(page)
            # ç™»å½•æ£€æŸ¥
            log("ç™»å½•æ£€æŸ¥ ...")
            ensure_logged_in(browser, page)

            for t_idx, target_url in enumerate(target_list, 1):
                try:
                    page.goto(target_url, wait_until='load')
                    log(f"æ‰“å¼€ç›®æ ‡[{t_idx}/{len(target_list)}]: {target_url}")

                    # å†æ¬¡ç¡®ä¿æ³¨å…¥ç”Ÿæ•ˆï¼ˆé’ˆå¯¹ä¸­é€”è·³è½¬ï¼‰
                    inject_api(page)

                    root = resolve_root(page, target_url)
                    files = list_tree(page, root['nodeId'])

                    # æ”¯æŒ adoc/axls å¯¼å‡ºï¼Œä»¥åŠ docx/xlsx/pdf åŸæ ¼å¼ç›´ä¸‹ï¼›ä¸”æ»¡è¶³æœ€å°æ›´æ–°æ—¶é—´ï¼ˆæ¯«ç§’è½¬ç§’æ¯”è¾ƒï¼‰
                    def _time_ok(rec: Dict[str, Any]) -> bool:
                        if min_ts is None:
                            return True
                        try:
                            ut = rec.get('updatedTime')
                            if ut is None:
                                return False
                            return int(int(ut) / 1000) >= int(min_ts)
                        except Exception:
                            return False

                    sel = [
                        f for f in files
                        if (
                            (f.get('extension') in ('adoc', 'axls') and f.get('docKey'))
                            or (f.get('extension') in ('docx', 'xlsx', 'pdf'))
                        ) and _time_ok(f)
                    ]
                    log(f"å¯¼å‡ºç›®æ ‡: {len(sel)}")

                    for idx, item in enumerate(sel, 1):
                        try:
                            log(f"[{idx}/{len(sel)}] {item['name']} -> {item.get('id')}")
                            export_and_download(page, browser, item, state, min_unix_ts=min_ts)
                            save_json(STATE_PATH, state)
                            print(f"[{idx}/{len(sel)}] OK: {item['id']}")
                        except Exception as e:
                            print(f"[{idx}/{len(sel)}] FAIL: {item.get('id')} -> {e}")
                            save_json(STATE_PATH, state)
                except Exception as e:
                    log(f"ç›®æ ‡å¤„ç†å¤±è´¥: {target_url}: {e}")
                    continue

        finally:
            browser.close()


if __name__ == '__main__':
    sys.exit(main())



